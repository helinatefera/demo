{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10989413,"sourceType":"datasetVersion","datasetId":6839925},{"sourceId":11137797,"sourceType":"datasetVersion","datasetId":6947106},{"sourceId":11182609,"sourceType":"datasetVersion","datasetId":6980161},{"sourceId":11206834,"sourceType":"datasetVersion","datasetId":6997562},{"sourceId":11218390,"sourceType":"datasetVersion","datasetId":7005762}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric\n!pip install multilingual-clip torch","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-04-01T06:33:52.947853Z","iopub.execute_input":"2025-04-01T06:33:52.948258Z","iopub.status.idle":"2025-04-01T06:34:02.276409Z","shell.execute_reply.started":"2025-04-01T06:33:52.948222Z","shell.execute_reply":"2025-04-01T06:34:02.275257Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.12)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.12.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: multilingual-clip in /usr/local/lib/python3.10/dist-packages (1.0.10)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from multilingual-clip) (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->multilingual-clip) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->multilingual-clip) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->multilingual-clip) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->multilingual-clip) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->multilingual-clip) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->multilingual-clip) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers->multilingual-clip) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->multilingual-clip) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers->multilingual-clip) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers->multilingual-clip) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers->multilingual-clip) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers->multilingual-clip) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers->multilingual-clip) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\nimport torch.optim as optim\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-04-01T06:34:02.278566Z","iopub.execute_input":"2025-04-01T06:34:02.279062Z","iopub.status.idle":"2025-04-01T06:34:34.254252Z","shell.execute_reply.started":"2025-04-01T06:34:02.279001Z","shell.execute_reply":"2025-04-01T06:34:34.253288Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nimport pickle\n\n# Path to the saved pickle file\nval = \"/kaggle/input/msdv-feat-16/val_16_feat.pkl\"\ntrain  = \"/kaggle/input/msdv-feat-16/train_16_feat.pkl\"\ntest = \"/kaggle/input/msdv-feat-16/test_16_feat.pkl\"\n# Load the features\nwith open(val, \"rb\") as f:\n    val_graph_features = pickle.load(f)\nwith open(train, \"rb\") as f:\n    train_graph_features = pickle.load(f)\nwith open(test, \"rb\") as f:\n    test_graph_features = pickle.load(f)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val = \"/kaggle/working/val_am_graph_embeddings.pkl\"\nwith open(val, \"rb\") as f:\n    val_graph_features = pickle.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_graph_features['bQJQGoJF7_k_162_169']","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n---","metadata":{}},{"cell_type":"markdown","source":"## BERT embedding","metadata":{}},{"cell_type":"markdown","source":"---\n---","metadata":{}},{"cell_type":"markdown","source":"# **Video-Question representation** \n### video feature have quetion understanding","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import MessagePassing\nfrom tqdm import tqdm\nimport os\n\n# Load question-answer CSV\nqa_df = pd.read_csv('/kaggle/input/final-am-qa/val_am_updated_file.csv')\n\n# Load feature file\nwith open('/kaggle/input/msdv-feat-16/val_16_feat.pkl', 'rb') as f:\n    video_data = pickle.load(f)\n\n# Tokenizer and language model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Define STGNN layer with attention mechanism\nclass AttentionSTGNNLayer(MessagePassing):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__(aggr='add')\n        self.key = torch.nn.Linear(in_channels, hidden_channels)\n        self.query = torch.nn.Linear(in_channels, hidden_channels)\n        self.value = torch.nn.Linear(in_channels, hidden_channels)\n        self.out = torch.nn.Linear(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index):\n        return self.propagate(edge_index, x=x)\n\n    def message(self, x_i, x_j):\n        attn_score = (self.query(x_i) * self.key(x_j)).sum(dim=-1, keepdim=True)\n        attn_weight = torch.sigmoid(attn_score)\n        return attn_weight * self.value(x_j)\n\n    def update(self, aggr_out):\n        return F.relu(self.out(aggr_out))\n\n# Initialize layers\nstgnn1 = AttentionSTGNNLayer(768, 512, 512)\nstgnn2 = AttentionSTGNNLayer(512, 256, 256)\n\n# Prepare projection layers after inferring input dimensions\nsample_video = next(iter(video_data.values()))\nsample_tensor = torch.tensor(sample_video['object_features'][0]) if not isinstance(sample_video['object_features'][0], torch.Tensor) else sample_video['object_features'][0]\nfeat_dim = sample_tensor.size(1)\nproj_obj = torch.nn.Linear(feat_dim, 768)\nproj_q = torch.nn.Linear(768, 768)\n\n# Load previously saved features if available\noutput_path = '/kaggle/working/val_am_graph_embeddings.pkl'\nif os.path.exists(output_path):\n    with open(output_path, 'rb') as f:\n        features_dict = pickle.load(f)\nelse:\n    features_dict = {}\n\nfor idx, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc=\"Processing videos\"):\n    video_id = row['video_id']\n    question = row['question']\n\n    if video_id in features_dict:\n        continue\n\n    if not isinstance(question, str) or not question.strip():\n        continue\n\n    if video_id not in video_data:\n        continue\n\n    object_features = video_data[video_id]['object_features']\n\n    inputs = tokenizer(question, return_tensors='pt')\n    with torch.no_grad():\n        question_feat = bert_model(**inputs).last_hidden_state[:, 0, :]\n\n    if question_feat.size(1) == 0:\n        continue\n\n    q_feat_proj = proj_q(question_feat)\n\n    nodes = []\n    edge_index = []\n    node_offset = 0\n    frame_node_counts = []\n\n    for t, frame_feats in enumerate(object_features):\n        frame_feats = torch.tensor(frame_feats) if not isinstance(frame_feats, torch.Tensor) else frame_feats\n        if frame_feats.size(0) == 0:\n            frame_node_counts.append(0)\n            continue\n\n        num_objs = frame_feats.size(0)\n        obj_proj = proj_obj(frame_feats)\n        q_proj = q_feat_proj.repeat(num_objs, 1)\n        combined_feat = obj_proj + q_proj\n        nodes.append(combined_feat)\n\n        indices = torch.arange(num_objs) + node_offset\n        spatial = torch.combinations(indices, r=2).T\n        edge_index.append(torch.cat([spatial, spatial[[1, 0]]], dim=1))\n\n        if len(frame_node_counts) > 0 and frame_node_counts[-1] > 0:\n            prev_num = frame_node_counts[-1]\n            curr_idx = torch.arange(num_objs) + node_offset\n            prev_idx = torch.arange(prev_num) + node_offset - prev_num\n            temporal = torch.cartesian_prod(prev_idx, curr_idx).T\n            edge_index.append(temporal)\n\n        frame_node_counts.append(num_objs)\n        node_offset += num_objs\n\n    if not nodes:\n        continue\n\n    x = torch.cat(nodes, dim=0)\n    edge_index = torch.cat(edge_index, dim=1)\n    data = Data(x=x, edge_index=edge_index)\n\n    x = stgnn1(data.x, data.edge_index)\n    x = stgnn2(x, data.edge_index)\n    graph_embedding = x.mean(dim=0)\n\n    features_dict[video_id] = graph_embedding.cpu()\n\n    # Save after each video\n    with open(output_path, 'wb') as f:\n        pickle.dump(features_dict, f)\n\nprint(\"Saved graph-level features for all videos.\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-30T09:40:38.169002Z","iopub.execute_input":"2025-03-30T09:40:38.169411Z","iopub.status.idle":"2025-03-30T09:40:39.854964Z","shell.execute_reply.started":"2025-03-30T09:40:38.169380Z","shell.execute_reply":"2025-03-30T09:40:39.853916Z"}},"outputs":[{"name":"stderr","text":"Processing videos: 100%|██████████| 6415/6415 [00:00<00:00, 22573.10it/s]","output_type":"stream"},{"name":"stdout","text":"Saved graph-level features for all videos.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# ***Training***","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------\n# Load Data\n# --------------------\nqa_df = pd.read_csv('/kaggle/input/final-am-qa/train_am_updated_file.csv')\n\nwith open('/kaggle/input/vit-feature/train_vit_yolo_video_features.pkl', 'rb') as f:\n    train_feature_data = pickle.load(f)\nwith open('/kaggle/input/vit-feature/val_vit_yolo_video_features.pkl', 'rb') as f:\n    val_feature_data = pickle.load(f)\nwith open('/kaggle/input/vit-feature/test_vit_yolo_video_features.pkl', 'rb') as f:\n    test_feature_data = pickle.load(f)\n\nwith open('/kaggle/input/embedding/train_graph_embeddings.pkl', 'rb') as f:\n    train_graph_features = pickle.load(f)\nwith open('/kaggle/input/embedding/val_graph_embeddings.pkl', 'rb') as f:\n    val_graph_features = pickle.load(f)\nwith open('/kaggle/input/embedding/test_graph_embeddings.pkl', 'rb') as f:\n    test_graph_features = pickle.load(f)\n\n# --------------------\n# Video Splits\n# --------------------\ntrain_ids = set(train_feature_data.keys())\nval_ids = set(val_feature_data.keys())\ntest_ids = set(test_feature_data.keys())\n\n# --------------------\n# Text Encoder (BERT)\n# --------------------\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_model.to(device)\nbert_model.eval()\n\n# --------------------\n# Label Encode Answers\n# --------------------\nqa_df = qa_df[qa_df['video_id'].isin(train_ids | val_ids | test_ids) & \n              qa_df['question'].notnull() & \n              qa_df['answer'].notnull()]\nlabel_encoder = LabelEncoder()\nqa_df['label'] = label_encoder.fit_transform(qa_df['answer'])\n\n# ----------------------------\n# Positional Encoding Function\n# ----------------------------\ndef get_positional_encoding(length, dim):\n    position = torch.arange(0, length, dtype=torch.float, device=device).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, dim, 2, device=device).float() * (-torch.log(torch.tensor(10000.0, device=device)) / dim))\n    pe = torch.zeros(length, dim, device=device)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe\n\n# ----------------------------\n# Dataset Definition\n# ----------------------------\nclass QAGraphDataset(Dataset):\n    def __init__(self, df, graph_feats, full_feats):\n        self.df = df.reset_index(drop=True)\n        self.graph_feats = graph_feats\n        self.full_feats = full_feats\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        vid = row['video_id']\n        question = row['question']\n        label = row['label']\n\n        if vid not in self.graph_feats or vid not in self.full_feats:\n            return torch.zeros(1, 1536, device=device), torch.tensor(-1, device=device)\n\n        with torch.no_grad():\n            inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            q_feat = bert_model(**inputs).last_hidden_state.squeeze(0)  # [L, 768]\n\n        L = q_feat.size(0)\n        pos_embed = get_positional_encoding(L, q_feat.size(1))\n        q_feat = q_feat + pos_embed\n\n        g_feat = self.graph_feats[vid].to(device).unsqueeze(0).repeat(L, 1)  # [L, 256]\n\n        frame_feats = self.full_feats[vid]['vit']\n        if isinstance(frame_feats, list):\n            frame_feats = [torch.tensor(f, device=device) if not isinstance(f, torch.Tensor) else f.to(device) for f in frame_feats]\n            frame_feats = torch.cat(frame_feats, dim=0)\n        if frame_feats.dim() == 3:\n            frame_feats = frame_feats.squeeze(1)\n        else:\n            frame_feats = frame_feats.to(device)\n        f_feat = frame_feats.mean(dim=0).unsqueeze(0).repeat(L, 1)\n\n        fusion = torch.cat([g_feat, f_feat, q_feat], dim=-1)  # [L, 1536]\n        return fusion, torch.tensor(label, device=device)\n\n# ----------------------------\n# Collate Function\n# ----------------------------\ndef collate_fn(batch):\n    filtered = [(x, y) for x, y in batch if y.item() >= 0]\n    if not filtered:\n        return torch.zeros(1, 1, 1536, device=device), torch.tensor([-1], device=device)\n    x_seqs, labels = zip(*filtered)\n    x_padded = pad_sequence(x_seqs, batch_first=True)\n    y = torch.stack(labels)\n    return x_padded, y\n\n# ----------------------------\n# Dataset Split\n# ----------------------------\ntrain_df = qa_df[qa_df['video_id'].isin(train_ids)]\nval_df = qa_df[qa_df['video_id'].isin(val_ids)]\ntest_df = qa_df[qa_df['video_id'].isin(test_ids)]\n\ntrain_loader = DataLoader(QAGraphDataset(train_df, train_graph_features, train_feature_data), \n                          batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(QAGraphDataset(val_df, val_graph_features, val_feature_data), \n                        batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(QAGraphDataset(test_df, test_graph_features, test_feature_data), \n                         batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# ----------------------------\n# Model: Attention Classifier\n# ----------------------------\nclass AttentionClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim=input_dim, num_heads=4, batch_first=True)\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, num_classes)\n        )\n\n    def forward(self, x):\n        attn_output, _ = self.attn(x, x, x)\n        pooled = attn_output.mean(dim=1)\n        return self.mlp(pooled)\n\nmodel = AttentionClassifier(1792, 512, len(label_encoder.classes_))\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nstart_epoch = 1\ncheckpoint_path = \"/kaggle/working/checkpoint.pth\"\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming training from epoch {start_epoch} with saved loss {checkpoint['loss']:.4f}\")\n\ndef evaluate(model, loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for x, y in loader:\n            if y[0] < 0:\n                continue\n            out = model(x)\n            correct += (out.argmax(dim=1) == y).sum().item()\n            total += x.size(0)\n    return 100. * correct / total if total > 0 else 0.0\n\nfor epoch in range(start_epoch, 21):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    for x, y in loop:\n        if y[0] < 0:\n            continue\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n        correct += (out.argmax(dim=1) == y).sum().item()\n        total += x.size(0)\n        loop.set_postfix(loss=total_loss / total, acc=100. * correct / total)\n\n    val_acc = evaluate(model, val_loader)\n    print(f\"Epoch {epoch} complete | Train Loss: {total_loss/total:.4f} | Train Acc: {100. * correct/total:.2f}% | Val Acc: {val_acc:.2f}%\")\n\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': total_loss / total,\n    }\n    torch.save(checkpoint, checkpoint_path)\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-04-01T06:34:34.255691Z","iopub.execute_input":"2025-04-01T06:34:34.256470Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ef867964274bb3bcc5569064ecd58e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e6cf19cce0465bbd725a3b81fc4f96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a775a6644d48138db2e84f71a12183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"441d1250627640b4a1b6ce8d62d176dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528f5463de4b46beb752e2dae431c9c5"}},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|██████████| 967/967 [36:03<00:00,  2.24s/it, acc=19.8, loss=4.99]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 complete | Train Loss: 4.9883 | Train Acc: 19.76% | Val Acc: 0.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  64%|██████▍   | 622/967 [23:33<13:03,  2.27s/it, acc=24.7, loss=4.16]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/vit-feature/val_vit_yolo_video_features.pkl', 'rb') as f:\n    val_feature_data = pickle.load(f)\nval_feature_data['bQJQGoJF7_k_162_169']['vit'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:52:39.033659Z","iopub.execute_input":"2025-03-30T20:52:39.034013Z","iopub.status.idle":"2025-03-30T20:52:39.067827Z","shell.execute_reply.started":"2025-03-30T20:52:39.033987Z","shell.execute_reply":"2025-03-30T20:52:39.066862Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 1, 768])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport os\n\n# --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nqa_df = pd.read_csv('/kaggle/input/final-am-qa/train_am_updated_file.csv')\n\nwith open('/kaggle/input/msdv-feat-16/train_16_feat.pkl', 'rb') as f:\n    train_feature_data = pickle.load(f)\nwith open('/kaggle/input/msdv-feat-16/val_16_feat.pkl', 'rb') as f:\n    val_feature_data = pickle.load(f)\nwith open('/kaggle/input/msdv-feat-16/test_16_feat.pkl', 'rb') as f:\n    test_feature_data = pickle.load(f)\n\nwith open('/kaggle/working/train_am_graph_embeddings.pkl', 'rb') as f:\n    train_graph_features = pickle.load(f)\nwith open('/kaggle/working/val_am_graph_embeddings.pkl', 'rb') as f:\n    val_graph_features = pickle.load(f)\nwith open('/kaggle/working/test_am_graph_embeddings.pkl', 'rb') as f:\n    test_graph_features = pickle.load(f)\n\ntrain_ids = set(train_feature_data.keys())\nval_ids = set(val_feature_data.keys())\ntest_ids = set(test_feature_data.keys())\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_model.to(device)\nbert_model.eval()\n\nqa_df = qa_df[qa_df['video_id'].isin(train_ids | val_ids | test_ids) & \n              qa_df['question'].notnull() & \n              qa_df['answer'].notnull()]\nlabel_encoder = LabelEncoder()\nqa_df['label'] = label_encoder.fit_transform(qa_df['answer'])\n\ndef get_positional_encoding(length, dim):\n    position = torch.arange(0, length, dtype=torch.float, device=device).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, dim, 2, device=device).float() * (-torch.log(torch.tensor(10000.0, device=device)) / dim))\n    pe = torch.zeros(length, dim, device=device)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe\n\nclass QAGraphDataset(Dataset):\n    def __init__(self, df, graph_feats, full_feats):\n        self.df = df.reset_index(drop=True)\n        self.graph_feats = graph_feats\n        self.full_feats = full_feats\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        vid = row['video_id']\n        question = row['question']\n        label = row['label']\n\n        if vid not in self.graph_feats or vid not in self.full_feats:\n            return torch.zeros(1, 1536, device=device), torch.tensor(-1, device=device)\n\n        with torch.no_grad():\n            inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            q_feat = bert_model(**inputs).last_hidden_state.squeeze(0)\n\n        L = q_feat.size(0)\n        pos_embed = get_positional_encoding(L, q_feat.size(1))\n        q_feat = q_feat + pos_embed\n\n        g_feat = self.graph_feats[vid].to(device).unsqueeze(0).repeat(L, 1)\n\n        frame_feats = self.full_feats[vid]['frame_features']\n        if isinstance(frame_feats, list):\n            frame_feats = [torch.tensor(f, device=device) if not isinstance(f, torch.Tensor) else f.to(device) for f in frame_feats]\n            frame_feats = torch.cat(frame_feats, dim=0)\n        if frame_feats.dim() == 3:\n            frame_feats = frame_feats.squeeze(1)\n        else:\n            frame_feats = frame_feats.to(device)\n        f_feat = frame_feats.mean(dim=0).unsqueeze(0).repeat(L, 1)\n\n        fusion = torch.cat([g_feat, f_feat, q_feat], dim=-1)\n        return fusion, torch.tensor(label, device=device)\n\ndef collate_fn(batch):\n    filtered = [(x, y) for x, y in batch if y.item() >= 0]\n    if not filtered:\n        return torch.zeros(1, 1, 1536, device=device), torch.tensor([-1], device=device)\n    x_seqs, labels = zip(*filtered)\n    x_padded = pad_sequence(x_seqs, batch_first=True)\n    y = torch.stack(labels)\n    return x_padded, y\n\ntrain_df = qa_df[qa_df['video_id'].isin(train_ids)]\nval_df = qa_df[qa_df['video_id'].isin(val_ids)]\ntest_df = qa_df[qa_df['video_id'].isin(test_ids)]\n\ntrain_loader = DataLoader(QAGraphDataset(train_df, train_graph_features, train_feature_data), batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(QAGraphDataset(val_df, val_graph_features, val_feature_data), batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(QAGraphDataset(test_df, test_graph_features, test_feature_data), batch_size=32, shuffle=False, collate_fn=collate_fn)\n\nclass BidafAttn(torch.nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.linear = torch.nn.Linear(3 * input_dim, 1)\n\n    def forward(self, x1, x2):\n        B, L1, D = x1.size()\n        L2 = x2.size(1)\n        x1_exp = x1.unsqueeze(2).expand(B, L1, L2, D)\n        x2_exp = x2.unsqueeze(1).expand(B, L1, L2, D)\n        concat = torch.cat([x1_exp, x2_exp, x1_exp * x2_exp], dim=-1)\n        S = self.linear(concat).squeeze(-1)\n        A = torch.nn.functional.softmax(S, dim=-1)\n        attended = torch.bmm(A, x2)\n        return attended\n\nclass BidafClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.attn = BidafAttn(input_dim)\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(input_dim * 2, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, num_classes)\n        )\n\n    def forward(self, x):\n        attn_output = self.attn(x, x)\n        fusion = torch.cat([x, attn_output], dim=-1)\n        pooled = fusion.mean(dim=1)\n        return self.mlp(pooled)\n\nmodel = BidafClassifier(256 + 512 + 768, 512, len(label_encoder.classes_))\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nstart_epoch = 1\ncheckpoint_path = \"/kaggle/working/bi_checkpoint.pth\"\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming training from epoch {start_epoch} with saved loss {checkpoint['loss']:.4f}\")\n\ndef evaluate(model, loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for x, y in loader:\n            if y[0] < 0:\n                continue\n            out = model(x)\n            correct += (out.argmax(dim=1) == y).sum().item()\n            total += x.size(0)\n    return 100. * correct / total if total > 0 else 0.0\n\nfor epoch in range(start_epoch, 21):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    for x, y in loop:\n        if y[0] < 0:\n            continue\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n        correct += (out.argmax(dim=1) == y).sum().item()\n        total += x.size(0)\n        loop.set_postfix(loss=total_loss / total, acc=100. * correct / total)\n\n    val_acc = evaluate(model, val_loader)\n    print(f\"Epoch {epoch} complete | Train Loss: {total_loss/total:.4f} | Train Acc: {100. * correct/total:.2f}% | Val Acc: {val_acc:.2f}%\")\n\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': total_loss / total,\n    }\n    torch.save(checkpoint, checkpoint_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:45:44.797820Z","iopub.status.idle":"2025-03-30T20:45:44.798297Z","shell.execute_reply":"2025-03-30T20:45:44.798106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !cp /kaggle/input/embedding/* /kaggle/working/\n# ----------------------------\n# Save Trained Model\n# ----------------------------\ntorch.save(model.state_dict(), \"attention_classifier.pth\")\nwith open('val_graph_embeddings.pkl', 'rb') as f:\n    test_graph_features = pickle.load(f)\nwith open('/kaggle/input/msdv-feat-16/val_16_feat.pkl', 'rb') as f:\n    val_feature_data = pickle.load(f)\n# ----------------------------\n# Prediction Function for New Video\n# ----------------------------\ndef predict_answer(model, video_id, question, graph_features, full_feature_data, device):\n    model.eval()\n    with torch.no_grad():\n        # Move model to the specified device\n        model.to(device)\n        \n        # Tokenize the question and move input tensors to the same device as the model\n        inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True).to(device)\n        q_feat = bert_model(**inputs).last_hidden_state.squeeze(0)  # [L, 768]\n        L = q_feat.size(0)\n        \n        # Ensure positional encoding is on the same device\n        pos_embed = get_positional_encoding(L, q_feat.size(1)).to(device)\n        q_feat = q_feat + pos_embed\n\n        # Retrieve and move graph features to the same device\n        g_feat = graph_features[video_id].unsqueeze(0).repeat(L, 1).to(device)\n\n        # Retrieve frame features and move to the same device\n        frame_feats = full_feature_data[video_id]['frame_features']\n        if isinstance(frame_feats, list):\n            frame_feats = [torch.tensor(f).to(device) if not isinstance(f, torch.Tensor) else f.to(device) for f in frame_feats]\n            frame_feats = torch.cat(frame_feats, dim=0)\n        if frame_feats.dim() == 3:\n            frame_feats = frame_feats.squeeze(1)\n        f_feat = frame_feats.mean(dim=0).unsqueeze(0).repeat(L, 1).to(device)\n\n        # Concatenate features and move to the same device\n        fusion = torch.cat([g_feat, f_feat, q_feat], dim=-1).unsqueeze(0).to(device)  # [1, L, 1536]\n        \n        # Compute logits and prediction\n        logits = model(fusion)\n        pred = logits.argmax(dim=1).item()\n        return label_encoder.inverse_transform([pred])[0]\n\n# Example usage:\nanswer = predict_answer(model, \"bQJQGoJF7_k_162_169\", \"what is someone doing?\", test_graph_features, val_feature_data,device)\nprint(\"Predicted answer:\", answer)\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n---\n---","metadata":{}},{"cell_type":"markdown","source":"## CLIP embedding","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nfrom multilingual_clip import pt_multilingual_clip\nimport transformers\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import MessagePassing\nfrom tqdm import tqdm\n\n# Load question-answer CSV\nqa_df = pd.read_csv('/kaggle/input/embedding/test_graph_embeddings.pkl')\n\n# Load feature file\nwith open('/kaggle/input/msdv-feat-16/test_16_feat.pkl', 'rb') as f:\n    video_data = pickle.load(f)\n\n# Load M-CLIP text encoder\nmodel_name = 'M-CLIP/XLM-Roberta-Large-Vit-B-32'\nclip_text_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\n# Define STGNN layer with attention mechanism\nclass AttentionSTGNNLayer(MessagePassing):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__(aggr='add')\n        self.key = torch.nn.Linear(in_channels, hidden_channels)\n        self.query = torch.nn.Linear(in_channels, hidden_channels)\n        self.value = torch.nn.Linear(in_channels, hidden_channels)\n        self.out = torch.nn.Linear(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index):\n        return self.propagate(edge_index, x=x)\n\n    def message(self, x_i, x_j):\n        attn_score = (self.query(x_i) * self.key(x_j)).sum(dim=-1, keepdim=True)\n        attn_weight = torch.sigmoid(attn_score)\n        return attn_weight * self.value(x_j)\n\n    def update(self, aggr_out):\n        return F.relu(self.out(aggr_out))\n\n# Initialize layers\nstgnn1 = AttentionSTGNNLayer(512, 512, 512)\nstgnn2 = AttentionSTGNNLayer(512, 256, 256)\n\n# Prepare projection layer based on object feature dimension\nsample_video = next(iter(video_data.values()))\nsample_tensor = torch.tensor(sample_video['object_features'][0]) if not isinstance(sample_video['object_features'][0], torch.Tensor) else sample_video['object_features'][0]\nfeat_dim = sample_tensor.size(1)\nproj_obj = torch.nn.Linear(feat_dim, 512)\n\n# Load previously saved features if available\noutput_path = '/kaggle/working/test_clip_graph_embeddings_am.pkl'\nif os.path.exists(output_path):\n    with open(output_path, 'rb') as f:\n        features_dict = pickle.load(f)\nelse:\n    features_dict = {}\n\nfor idx, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc=\"Processing videos\"):\n    video_id = row['video_id']\n    question = row['question']\n\n    if video_id in features_dict:\n        continue\n\n    if not isinstance(question, str) or not question.strip():\n        continue\n\n    if video_id not in video_data:\n        continue\n\n    object_features = video_data[video_id]['object_features']\n\n    with torch.no_grad():\n        question_feat = clip_text_model.forward([question], tokenizer)  # shape: [1, 512]\n\n    if question_feat.size(1) == 0:\n        continue\n\n    q_feat_proj = question_feat  # already 512-dim\n\n    nodes = []\n    edge_index = []\n    node_offset = 0\n    frame_node_counts = []\n\n    for t, frame_feats in enumerate(object_features):\n        frame_feats = torch.tensor(frame_feats) if not isinstance(frame_feats, torch.Tensor) else frame_feats\n        if frame_feats.size(0) == 0:\n            frame_node_counts.append(0)\n            continue\n\n        num_objs = frame_feats.size(0)\n        obj_proj = proj_obj(frame_feats)\n        q_proj = q_feat_proj.repeat(num_objs, 1)\n        combined_feat = obj_proj + q_proj\n        nodes.append(combined_feat)\n\n        indices = torch.arange(num_objs) + node_offset\n        spatial = torch.combinations(indices, r=2).T\n        edge_index.append(torch.cat([spatial, spatial[[1, 0]]], dim=1))\n\n        if len(frame_node_counts) > 0 and frame_node_counts[-1] > 0:\n            prev_num = frame_node_counts[-1]\n            curr_idx = torch.arange(num_objs) + node_offset\n            prev_idx = torch.arange(prev_num) + node_offset - prev_num\n            temporal = torch.cartesian_prod(prev_idx, curr_idx).T\n            edge_index.append(temporal)\n\n        frame_node_counts.append(num_objs)\n        node_offset += num_objs\n\n    if not nodes:\n        continue\n\n    x = torch.cat(nodes, dim=0)\n    edge_index = torch.cat(edge_index, dim=1)\n    data = Data(x=x, edge_index=edge_index)\n\n    x = stgnn1(data.x, data.edge_index)\n    x = stgnn2(x, data.edge_index)\n    graph_embedding = x.mean(dim=0)\n\n    features_dict[video_id] = graph_embedding.cpu()\n\n    # Save after each video\n    with open(output_path, 'wb') as f:\n        pickle.dump(features_dict, f)\n\nprint(\"Saved graph-level features for all videos.\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------\n# Load Data\n# --------------------\nqa_df = pd.read_csv('/kaggle/input/msdv-qa-csv/train_q_a_f.csv')\n\nwith open('/kaggle/input/msdv-feat-16/train_16_feat.pkl', 'rb') as f:\n    train_feature_data = pickle.load(f)\nwith open('/kaggle/input/msdv-feat-16/val_16_feat.pkl', 'rb') as f:\n    val_feature_data = pickle.load(f)\nwith open('/kaggle/input/msdv-feat-16/test_16_feat.pkl', 'rb') as f:\n    test_feature_data = pickle.load(f)\n\nwith open('train_clip_graph_embeddings.pkl', 'rb') as f:\n    train_graph_features = pickle.load(f)\nwith open('val_clip_graph_embeddings.pkl', 'rb') as f:\n    val_graph_features = pickle.load(f)\nwith open('test_clip_graph_embeddings.pkl', 'rb') as f:\n    test_graph_features = pickle.load(f)\n\n# --------------------\n# Video Splits\n# --------------------\ntrain_ids = set(train_feature_data.keys())\nval_ids = set(val_feature_data.keys())\ntest_ids = set(test_feature_data.keys())\n\n# --------------------\n# Text Encoder (BERT)\n# --------------------\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_model.to(device)\nbert_model.eval()\n\n# --------------------\n# Label Encode Answers\n# --------------------\nqa_df = qa_df[qa_df['video_id'].isin(train_ids | val_ids | test_ids) & \n              qa_df['question'].notnull() & \n              qa_df['answer'].notnull()]\nlabel_encoder = LabelEncoder()\nqa_df['label'] = label_encoder.fit_transform(qa_df['answer'])\n\n# ----------------------------\n# Positional Encoding Function\n# ----------------------------\ndef get_positional_encoding(length, dim):\n    position = torch.arange(0, length, dtype=torch.float, device=device).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, dim, 2, device=device).float() * (-torch.log(torch.tensor(10000.0, device=device)) / dim))\n    pe = torch.zeros(length, dim, device=device)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe\n\n# ----------------------------\n# Dataset Definition\n# ----------------------------\nclass QAGraphDataset(Dataset):\n    def __init__(self, df, graph_feats, full_feats):\n        self.df = df.reset_index(drop=True)\n        self.graph_feats = graph_feats\n        self.full_feats = full_feats\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        vid = row['video_id']\n        question = row['question']\n        label = row['label']\n\n        if vid not in self.graph_feats or vid not in self.full_feats:\n            return torch.zeros(1, 1536, device=device), torch.tensor(-1, device=device)\n\n        with torch.no_grad():\n            inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            q_feat = bert_model(**inputs).last_hidden_state.squeeze(0)  # [L, 768]\n\n        L = q_feat.size(0)\n        pos_embed = get_positional_encoding(L, q_feat.size(1))\n        q_feat = q_feat + pos_embed\n\n        g_feat = self.graph_feats[vid].to(device).unsqueeze(0).repeat(L, 1)  # [L, 256]\n\n        frame_feats = self.full_feats[vid]['frame_features']\n        if isinstance(frame_feats, list):\n            frame_feats = [torch.tensor(f, device=device) if not isinstance(f, torch.Tensor) else f.to(device) for f in frame_feats]\n            frame_feats = torch.cat(frame_feats, dim=0)\n        if frame_feats.dim() == 3:\n            frame_feats = frame_feats.squeeze(1)\n        else:\n            frame_feats = frame_feats.to(device)\n        f_feat = frame_feats.mean(dim=0).unsqueeze(0).repeat(L, 1)\n\n        fusion = torch.cat([g_feat, f_feat, q_feat], dim=-1)  # [L, 1536]\n        return fusion, torch.tensor(label, device=device)\n\n# ----------------------------\n# Collate Function\n# ----------------------------\ndef collate_fn(batch):\n    filtered = [(x, y) for x, y in batch if y.item() >= 0]\n    if not filtered:\n        return torch.zeros(1, 1, 1536, device=device), torch.tensor([-1], device=device)\n    x_seqs, labels = zip(*filtered)\n    x_padded = pad_sequence(x_seqs, batch_first=True)\n    y = torch.stack(labels)\n    return x_padded, y\n\n# ----------------------------\n# Dataset Split\n# ----------------------------\ntrain_df = qa_df[qa_df['video_id'].isin(train_ids)]\nval_df = qa_df[qa_df['video_id'].isin(val_ids)]\ntest_df = qa_df[qa_df['video_id'].isin(test_ids)]\n\ntrain_loader = DataLoader(QAGraphDataset(train_df, train_graph_features, train_feature_data), \n                          batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(QAGraphDataset(val_df, val_graph_features, val_feature_data), \n                        batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(QAGraphDataset(test_df, test_graph_features, test_feature_data), \n                         batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# ----------------------------\n# Model: Attention Classifier\n# ----------------------------\nclass AttentionClassifier(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim=input_dim, num_heads=4, batch_first=True)\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, num_classes)\n        )\n\n    def forward(self, x):\n        attn_output, _ = self.attn(x, x, x)\n        pooled = attn_output.mean(dim=1)\n        return self.mlp(pooled)\n\nmodel = AttentionClassifier(256 + 512 + 768, 512, len(label_encoder.classes_))\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nstart_epoch = 1\ncheckpoint_path = \"/kaggle/working/clip_checkpoint.pth\"\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming training from epoch {start_epoch} with saved loss {checkpoint['loss']:.4f}\")\n\ndef evaluate(model, loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for x, y in loader:\n            if y[0] < 0:\n                continue\n            out = model(x)\n            correct += (out.argmax(dim=1) == y).sum().item()\n            total += x.size(0)\n    return 100. * correct / total if total > 0 else 0.0\n\nfor epoch in range(start_epoch, 31):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    for x, y in loop:\n        if y[0] < 0:\n            continue\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n        correct += (out.argmax(dim=1) == y).sum().item()\n        total += x.size(0)\n        loop.set_postfix(loss=total_loss / total, acc=100. * correct / total)\n\n    val_acc = evaluate(model, val_loader)\n    print(f\"Epoch {epoch} complete | Train Loss: {total_loss/total:.4f} | Train Acc: {100. * correct/total:.2f}% | Val Acc: {val_acc:.2f}%\")\n\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': total_loss / total,\n    }\n    torch.save(checkpoint, checkpoint_path)\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install magic-wormhole","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wormhole send /kaggle/working/clip_checkpoint.pth","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Object Tracking","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_iou(box1, box2):\n    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n    x1, y1, x2, y2 = box1\n    x1_p, y1_p, x2_p, y2_p = box2\n\n    # Compute intersection\n    inter_x1 = max(x1, x1_p)\n    inter_y1 = max(y1, y1_p)\n    inter_x2 = min(x2, x2_p)\n    inter_y2 = min(y2, y2_p)\n\n    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n    \n    # Compute areas\n    box1_area = (x2 - x1) * (y2 - y1)\n    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n    \n    # Compute IoU\n    iou = inter_area / float(box1_area + box2_area - inter_area)\n    return iou\n\ndef track_objects_across_frames(video_features, iou_threshold=0.5):\n    \"\"\"Assign tracking IDs to objects appearing across frames\"\"\"\n    tracking_info = {}\n    object_id = 1  # Initialize first object ID\n    \n    prev_frame_objects = {}  # Store objects in previous frame\n    \n    for frame_idx, (objects, boxes) in enumerate(zip(video_features[\"object_features\"], video_features[\"bounding_boxes\"])):\n        frame_objects = {}\n\n        for obj_idx, box in enumerate(boxes):\n            best_match_id = None\n            max_iou = iou_threshold  # Only assign if IoU is above threshold\n            \n            for prev_idx, prev_box in prev_frame_objects.items():\n                iou = compute_iou(box, prev_box)\n                if iou > max_iou:\n                    best_match_id = prev_idx\n                    max_iou = iou\n            \n            if best_match_id is not None:\n                frame_objects[obj_idx] = tracking_info[(frame_idx - 1, best_match_id)]\n            else:\n                frame_objects[obj_idx] = object_id\n                object_id += 1\n\n            tracking_info[(frame_idx, obj_idx)] = frame_objects[obj_idx]\n        \n        prev_frame_objects = {idx: box for idx, box in enumerate(boxes)}\n\n    return tracking_info\n    \ndef process_all_videos(all_video_features):\n    all_video_tracking_info = {}\n\n    # Iterate through all videos and track objects across frames\n    for video_id, video_features in all_video_features.items():\n        print(f\"Processing video: {video_id}\")\n        \n        # Track objects for the current video\n        tracking_info = track_objects_across_frames(video_features)\n\n        # Store the tracking information for the video\n        all_video_tracking_info[video_id] = tracking_info\n        print(\"added\")\n    return all_video_tracking_info\nval_video_tracking_info = process_all_videos(val_graph_features)\ntrain_video_tracking_info = process_all_videos(train_graph_features)\ntest_video_tracking_info = process_all_videos(test_graph_features)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_video_tracking_info['bQJQGoJF7_k_162_169']","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Graph Building","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef build_graph_features(video_features, tracking_info):\n    frame_features_list = video_features[\"frame_features\"]\n    object_features_list = video_features[\"object_features\"]\n    bounding_boxes = video_features[\"bounding_boxes\"]\n\n    num_frames = len(frame_features_list)\n    num_objects = sum(len(objects) for objects in object_features_list)  \n\n    # Initialize adjacency matrix and feature tensor\n    adjacency_matrix = np.zeros((num_objects, num_objects))\n    feature_tensor = []\n\n    object_index_map = {}  # Maps (frame_idx, obj_idx) → global object index\n    global_obj_idx = 0\n\n    for frame_idx, objects in enumerate(object_features_list):\n        for obj_idx, obj_feature in enumerate(objects):\n            feature_tensor.append(obj_feature)\n            object_index_map[(frame_idx, obj_idx)] = global_obj_idx\n            global_obj_idx += 1\n\n    feature_tensor = np.array(feature_tensor)  \n\n    # ---- SPATIAL EDGES (within the same frame) ----\n    for frame_idx in range(num_frames):\n        if not bounding_boxes[frame_idx]: \n            continue  # Skip empty frames\n\n        bbox_centers = [( (box[0] + box[2]) / 2, (box[1] + box[3]) / 2 ) for box in bounding_boxes[frame_idx]]\n        bbox_centers = np.array(bbox_centers)\n\n        if bbox_centers.shape[0] == 0:\n            continue  \n\n        dist_matrix = cdist(bbox_centers, bbox_centers)\n\n        # Lower threshold to 100 or more for more edges\n        for i in range(len(bbox_centers)):\n            for j in range(i + 1, len(bbox_centers)):\n                if dist_matrix[i, j] < 100:  # Increase threshold for better connectivity\n                    obj1 = object_index_map[(frame_idx, i)]\n                    obj2 = object_index_map[(frame_idx, j)]\n                    adjacency_matrix[obj1, obj2] = 1\n                    adjacency_matrix[obj2, obj1] = 1\n\n    # ---- TEMPORAL EDGES (linking objects across frames) ----\n    for (prev_frame, prev_obj), track_id in tracking_info.items():\n        for (curr_frame, curr_obj), curr_track_id in tracking_info.items():\n            if curr_frame == prev_frame + 1 and curr_track_id == track_id:  \n                obj1 = object_index_map[(prev_frame, prev_obj)]\n                obj2 = object_index_map[(curr_frame, curr_obj)]\n                adjacency_matrix[obj1, obj2] = 1\n                adjacency_matrix[obj2, obj1] = 1\n\n    return adjacency_matrix, feature_tensor\n","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_all_videos(video_object_tracking, video_object_feature):\n    all_video_graphs = {}  # Store graphs for each video\n    \n    for video_id in video_object_tracking.keys():\n        tracking_info = video_object_tracking[video_id]  # Object tracking data\n        video_features = video_object_feature[video_id]  # Extracted features\n\n        adjacency_matrix, feature_tensor = build_graph_features(video_features, tracking_info)\n        \n        all_video_graphs[video_id] = {\n            \"adjacency_matrix\": adjacency_matrix,\n            \"feature_matrix\": feature_tensor\n        }\n\n    return all_video_graphs\nval_graph = process_all_videos(val_video_tracking_info, val_graph_features)\ntrain_graph = process_all_videos(train_video_tracking_info, train_graph_features)\ntest_graph = process_all_videos(test_video_tracking_info, test_graph_features)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# all_video_graphs = process_all_videos(all_video_tracking_info, video_graph_features)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_graph['bQJQGoJF7_k_162_169']['feature_matrix'].shape","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_graph['bQJQGoJF7_k_162_169']['adjacency_matrix'].shape","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_video_graphs['bQJQGoJF7_k_162_169']","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Graph feature extraction using stgcnn","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# Load adjacency and feature matrices\nvideo_graph = val_graph['bQJQGoJF7_k_162_169']\nadj_matrix = video_graph['adjacency_matrix']\nfeat_matrix = video_graph['feature_matrix']","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoSTGNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(VideoSTGNN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.relu(self.conv1(x, edge_index))\n        x = self.conv2(x, edge_index)\n        return x.mean(dim=0)  # Aggregate all nodes to a single representation\n","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Convert adjacency matrix to edge list\nedge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)\n\n# Convert feature matrix to PyTorch tensor\nx = torch.tensor(feat_matrix, dtype=torch.float)\n\n# Create PyG graph object\ngraph_data = Data(x=x, edge_index=edge_index)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_dim = x.shape[1]  # Feature dimension from the graph\nstgnn = VideoSTGNN(input_dim=input_dim, hidden_dim=128, output_dim=256)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# for all the video\n### Now, let’s scale the pipeline to handle 100 videos, extracting spatio-temporal graph features (STGNN) for each video and processing corresponding questions.\n\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"import torch\nfrom torch_geometric.data import Data\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass VideoSTGNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(VideoSTGNN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.relu(self.conv1(x, edge_index))\n        x = self.conv2(x, edge_index)\n        return x.mean(dim=0)  # Aggregate all nodes into a single video representation\n\n# Model Definition\ninput_dim = 1000  \nstgnn = VideoSTGNN(input_dim=input_dim, hidden_dim=128, output_dim=256)\n\n# Initialize a list to store aggregated features\nval_video_features = []\ntest_video_features = []\ntrain_video_features = []\n\nfor video_id, video_data in val_graph.items():\n    adj_matrix = video_data['adjacency_matrix']\n    feat_matrix = video_data['feature_matrix']\n\n    # Skip videos with no edges or missing features\n    if adj_matrix.sum() == 0 or feat_matrix.shape[0] == 0:\n        print(f\"Skipping video {video_id} due to missing edges or features.\")\n        continue\n\n    # Convert adjacency matrix to edge list\n    edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)\n\n    # Convert feature matrix to PyTorch tensor\n    x = torch.tensor(feat_matrix, dtype=torch.float)\n\n    # Ensure edge index doesn't exceed available nodes\n    if edge_index.max() >= x.shape[0]:\n        print(f\"Skipping video {video_id} due to invalid edge index.\")\n        continue\n\n    print(f\"Processing Video: {video_id}\")\n    print(f\"Feature matrix shape: {x.shape}\")\n    print(f\"Edge index shape: {edge_index.shape}\")\n\n    # Create PyG graph object\n    graph_data = Data(x=x, edge_index=edge_index)\n\n    # Process graph through STGNN\n    with torch.no_grad():\n        video_feature = stgnn(graph_data).unsqueeze(0)  # Shape: (1, 256)\n\n    # Store the features\n    val_video_features.append((video_id, video_feature))\nfor video_id, video_data in train_graph.items():\n    adj_matrix = video_data['adjacency_matrix']\n    feat_matrix = video_data['feature_matrix']\n\n    # Skip videos with no edges or missing features\n    if adj_matrix.sum() == 0 or feat_matrix.shape[0] == 0:\n        print(f\"Skipping video {video_id} due to missing edges or features.\")\n        continue\n\n    # Convert adjacency matrix to edge list\n    edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)\n\n    # Convert feature matrix to PyTorch tensor\n    x = torch.tensor(feat_matrix, dtype=torch.float)\n\n    # Ensure edge index doesn't exceed available nodes\n    if edge_index.max() >= x.shape[0]:\n        print(f\"Skipping video {video_id} due to invalid edge index.\")\n        continue\n\n    print(f\"Processing Video: {video_id}\")\n    print(f\"Feature matrix shape: {x.shape}\")\n    print(f\"Edge index shape: {edge_index.shape}\")\n\n    # Create PyG graph object\n    graph_data = Data(x=x, edge_index=edge_index)\n\n    # Process graph through STGNN\n    with torch.no_grad():\n        video_feature = stgnn(graph_data).unsqueeze(0)  # Shape: (1, 256)\n\n    # Store the features\n    train_video_features.append((video_id, video_feature))\nfor video_id, video_data in test_graph.items():\n    adj_matrix = video_data['adjacency_matrix']\n    feat_matrix = video_data['feature_matrix']\n\n    # Skip videos with no edges or missing features\n    if adj_matrix.sum() == 0 or feat_matrix.shape[0] == 0:\n        print(f\"Skipping video {video_id} due to missing edges or features.\")\n        continue\n\n    # Convert adjacency matrix to edge list\n    edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)\n\n    # Convert feature matrix to PyTorch tensor\n    x = torch.tensor(feat_matrix, dtype=torch.float)\n\n    # Ensure edge index doesn't exceed available nodes\n    if edge_index.max() >= x.shape[0]:\n        print(f\"Skipping video {video_id} due to invalid edge index.\")\n        continue\n\n    print(f\"Processing Video: {video_id}\")\n    print(f\"Feature matrix shape: {x.shape}\")\n    print(f\"Edge index shape: {edge_index.shape}\")\n\n    # Create PyG graph object\n    graph_data = Data(x=x, edge_index=edge_index)\n\n    # Process graph through STGNN\n    with torch.no_grad():\n        video_feature = stgnn(graph_data).unsqueeze(0)  # Shape: (1, 256)\n\n    # Store the features\n    test_video_features.append((video_id, video_feature))\n# Print first 5 video features\nfor video_id, feature in test_video_features[:5]:\n    print(f\"Video ID: {video_id}, Feature Shape: {feature.shape}\")","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_video_features[97][1]","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Now that i have graph-based video embeddings and a CSV file with video IDs, questions, and answers, the next step is training a Video Q&A model.\n\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# Load the CSV file containing questions, answers, and video IDs\ntest_qa = pd.read_csv('/kaggle/input/msdv-qa-csv/test_q_a_f.csv')\ntrain_qa = pd.read_csv('/kaggle/input/msdv-qa-csv/train_q_a_f.csv')\nval_qa = pd.read_csv('/kaggle/input/msdv-qa-csv/val_q_a_f.csv')\n# Check the first few rows of the CSV\nprint(train_qa.head())","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_ids =[]\nfor i in range(97):\n    video_ids.append(val_video_features[i][0])","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_ids)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# --- Step 1: Read the mapping file ---\nmapping_file = \"/kaggle/input/msvd-q-and-a/youtube_mapping.txt\"  # Adjust path if needed\n\n# Build a dictionary mapping the numeric part of the short id to the exact id.\nmapping_dict = {}\nwith open(mapping_file, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        parts = line.strip().split()\n        if len(parts) >= 2:\n            exact_id, short_id = parts[0], parts[1]\n            # Remove \"vid\" prefix from the short id, e.g. \"vid1\" becomes \"1\"\n            num_part = short_id.replace(\"vid\", \"\")\n            mapping_dict[num_part] = exact_id\n\nprint(\"Mapping dictionary:\", mapping_dict)\n# Expected output, for example: {\"1\": \"-4wsuPCjDBc_5_15\", \"2\": \"-7KMZQEsJW4_205_208\", ...}\n\n# --- Step 2: Read the CSV file ---\ncsv_file = \"/kaggle/input/val-am-new/am_val_q_a (1).csv\"  # Adjust path if necessary\ndf = pd.read_csv(csv_file)\nprint(\"Original CSV video_ids:\")\nprint(df[\"video_id\"].unique())\n\n# --- Step 3: Replace the video_id in CSV using the mapping ---\n# Assume that the CSV's video_id column contains values like \"1\", \"2\", etc.\ndf[\"video_id\"] = df[\"video_id\"].astype(str).map(mapping_dict)\n\n# --- Step 4: Save the updated CSV ---\noutput_csv_file = \"qa_data_updated.csv\"  # New file name for the updated CSV\ndf.to_csv(output_csv_file, index=False)\nprint(f\"Updated CSV saved to {output_csv_file}\")\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_qa.columns = train_qa.columns.str.strip()  # Remove any leading or trailing spaces in column names\ntest_qa.columns = test_qa.columns.str.strip() \nval_qa.columns = val_qa.columns.str.strip() \n\n# Merge all three datasets\nmerged_qa = pd.concat([train_qa, test_qa, val_qa], ignore_index=True)\n\n# Check the first few rows\nprint(len(merged_qa))","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the list of video_ids that are present in the DataFrame\nmatching_video_ids = df[df['video_id'].isin(video_ids)]\nprint(len(matching_video_ids['video_id'].unique()))","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is trained with the **answer labels** as the target variable. The answers are encoded into numerical labels using the **LabelEncoder**. Here's a summary of how the training process works:\n\n1. **Question-Video Feature Concatenation**:  \n   - Each **video embedding** and **question embedding** (from BERT) are concatenated into a single feature vector. The feature size is the combination of the 256-dimensional video feature and the 768-dimensional question embedding, resulting in a 1024-dimensional feature vector.\n\n2. **Training Target**:  \n   - The **answer** for each video-question pair is encoded into a numerical label using `LabelEncoder`. This label is the target variable `y` that the model will predict.\n\n3. **Model Prediction**:  \n   - The model (a simple **MLP** in this case) is trained to predict the **answer label** based on the concatenated video-question feature.\n\n4. **Cross-Entropy Loss**:  \n   - The **CrossEntropyLoss** is used, which is suitable for multi-class classification. The model is learning to output a probability distribution over all possible answers, and the goal is to minimize the loss between the predicted and actual answer labels.","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"val_video_embeddings_dict = {video_id: feature for video_id, feature in val_video_features}\ntrain_video_embeddings_dict = {video_id: feature for video_id, feature in train_video_features}\ntest_video_embeddings_dict = {video_id: feature for video_id, feature in test_video_features}","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode question function using BERT\ndef encode_question(question):\n    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=50)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state[:, 0, :]  # CLS token embedding (768D)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress tracking\n\n# Merge the dictionaries\nvideo_embeddings_dict = {\n    **train_video_embeddings_dict, \n    **val_video_embeddings_dict, \n    **test_video_embeddings_dict\n}\n\n# Prepare training data\nX, y = [], []\nfor index, row in tqdm(merged_qa.iterrows(), total=len(merged_qa), desc=\"Processing QA Pairs\"):\n    video_id, question, answer = row[\"video_id\"], row[\"question\"], row[\"answer\"]\n    \n    # Ensure video embedding exists\n    if video_id not in video_embeddings_dict:\n        continue\n    \n    # Get video embedding (256D)\n    video_embedding = video_embeddings_dict[video_id]\n\n    # Ensure video embedding is a tensor\n    if isinstance(video_embedding, list):\n        video_embedding = torch.tensor(video_embedding, dtype=torch.float32)\n    \n    # Encode question (768D)\n    question_embedding = encode_question(question)\n\n    # Ensure question embedding is a tensor\n    if isinstance(question_embedding, list):\n        question_embedding = torch.tensor(question_embedding, dtype=torch.float32)\n    \n    # Ensure both tensors have the same dimension format\n    if video_embedding.dim() == 1:\n        video_embedding = video_embedding.unsqueeze(0)  # Convert to (1, 256)\n    if question_embedding.dim() == 1:\n        question_embedding = question_embedding.unsqueeze(0)  # Convert to (1, 768)\n\n    # Concatenate video and question embeddings → (1, 1024)\n    combined_feature = torch.cat((video_embedding, question_embedding), dim=1)\n\n    X.append(combined_feature)\n    y.append(answer)  # Store corresponding answer label","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y[0],X[0],X[0].shape","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to tensor\nX = torch.vstack(X)  # Shape: (num_samples, 1024)\nX[0].shape","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode answer labels (this turns answers into numerical values)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert answers to PyTorch tensor\ny_tensor = torch.tensor(y_encoded, dtype=torch.long)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_tensor.shape","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataLoader for batching\ndataset = TensorDataset(X, y_tensor)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the Video Q&A model (MLP)\nclass VideoQAModel(nn.Module):\n    def __init__(self, input_dim=1024, hidden_dim=512, num_classes=len(label_encoder.classes_)):\n        super(VideoQAModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model\nvqa_model = VideoQAModel()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vqa_model.parameters(), lr=0.001)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for inputs, labels in dataloader:\n        optimizer.zero_grad()\n        outputs = vqa_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Predict answer for a new video-question pair\ntest_video_id = \"bQJQGoJF7_k_162_169\"  # Replace with actual video ID\ntest_question = \"ስጋን የሚጨምር ማን ነው?\"\n\n# Get video embedding for the test video\ntest_video_embedding = video_embeddings_dict[test_video_id]\n\n# Encode the question\ntest_question_embedding = encode_question(test_question)\n\n# Concatenate features for prediction (256D + 768D = 1024D)\ntest_input = torch.cat((test_video_embedding, test_question_embedding), dim=1)\n\n# Predict answer\nwith torch.no_grad():\n    output = vqa_model(test_input)\n    predicted_label = torch.argmax(output).item()\n\n# Convert predicted label to answer text\npredicted_answer = label_encoder.inverse_transform([predicted_label])[0]\nprint(f\"Predicted Answer: {predicted_answer}\")","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"evaluate your VQA system's performance on these videos using the available ground truth question-answer pairs. Here's how you can set up the evaluation","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# Function to get the predicted answer for a video\ndef predict_answer(video_features):\n    # Replace this with your model prediction code\n    return vqa_model(video_features)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a dictionary to store the aggregated features for each video\nvideo_features_new = {}\n\n# Loop through each video in the all_video_graphs dictionary\nfor video_id, video_data in all_video_graphs.items():\n    # Extract the adjacency and feature matrices for the current video\n    adj_matrix = video_data['adjacency_matrix']\n    feat_matrix = video_data['feature_matrix']\n        if adj_matrix.sum() == 0 or feat_matrix.shape[0] == 0:\n        print(f\"Skipping video {video_id} due to missing edges or features.\")\n        continue\n\n    # Convert adjacency matrix to edge list\n    edge_index = torch.tensor(adj_matrix.nonzero(), dtype=torch.long)\n\n    # Convert feature matrix to PyTorch tensor\n    x = torch.tensor(feat_matrix, dtype=torch.float)\n    # Create PyG graph object\n    graph_data = Data(x=x, edge_index=edge_index)\n    \n    # Pass the graph data through the STGNN model\n        # Process graph through STGNN\n    with torch.no_grad():\n        video_feature = stgnn(graph_data).unsqueeze(0)  # Shape: (1, 256)\n    \n    # Store the resulting aggregated features for each video in the dictionary\n    video_features_new[video_id] = video_feature\n","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_features_new['bQJQGoJF7_k_162_169']","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to calculate Exact Match score\ndef exact_match(predicted_answer, ground_truth_answer):\n    return predicted_answer.strip().lower() == ground_truth_answer.strip().lower()\n\n# Calculate Exact Match for all examples\nexact_match_count = 0\ntotal_count = len(df)\n\nfor idx, row in df.iterrows():\n    video_id = row['video_id']\n    question = row['question']\n    ground_truth_answer = row['answer']\n    print(video_id)\n    # Get the predicted answer (using the model)\n    predicted_answer = predict_answer(video_features_new[video_id])\n\n    # Check if the prediction is an exact match\n    if exact_match(predicted_answer, ground_truth_answer):\n        exact_match_count += 1\n\n# Calculate Exact Match (EM) score\nexact_match_score = exact_match_count / total_count\nprint(f\"Exact Match Score: {exact_match_score * 100:.2f}%\")","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null}]}