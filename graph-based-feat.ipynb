{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3051927,"sourceType":"datasetVersion","datasetId":1868590},{"sourceId":10901013,"sourceType":"datasetVersion","datasetId":6775006},{"sourceId":10970964,"sourceType":"datasetVersion","datasetId":6826441}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-geometric\n!pip install ultralytics\n!!pip install git+https://github.com/openai/CLIP.git\n!pip install detectron2\n!pip install yolov5","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"steveandreasimmanuel/msvd-video-caption\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nprint(\"Number of GPUs available:\", torch.cuda.device_count())\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nlogging.getLogger(\"ultralytics\").setLevel(logging.CRITICAL)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm /kaggle/working/testing_all_video_features.pkl","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP with yolo","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom PIL import Image\nfrom torchvision import models, transforms\nimport pickle\nimport clip\nfrom ultralytics import YOLO\nfrom tqdm import tqdm  # Import tqdm for progress bar\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Load CLIP model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device)\n\n# Load YOLO model\nyolo_model = YOLO(\"yolov8s.pt\")  \n\n# Load ResNet for object feature extraction\nresnet = models.resnet50(pretrained=True)\nresnet.eval()\nfasterrcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\nfasterrcnn_model.eval()\n# Transformation for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Function to extract frame-level features using CLIP\ndef extract_frame_features_from_frame(frame):\n    frame_tensor = preprocess(frame).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features = clip_model.encode_image(frame_tensor)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n    return image_features.cpu().numpy()\ndef extract_object_features_from_frame(frame):\n    object_features = []\n    bounding_boxes = []\n    \n    # Convert image to tensor and move it to the device (GPU)\n    frame_tensor = transforms.ToTensor()(frame).unsqueeze(0).to(device)  # Move tensor to GPU\n    \n    # Ensure the Faster R-CNN and ResNet models are on the GPU\n    fasterrcnn_model.to(device)\n    resnet.to(device)\n    \n    with torch.no_grad():\n        detections = fasterrcnn_model(frame_tensor)[0]\n    \n    for i in range(len(detections['boxes'])):\n        score = detections['scores'][i].item()\n        \n        if score > 0.3:  # Lower the threshold to capture more objects\n            x1, y1, x2, y2 = map(int, detections['boxes'][i].cpu().numpy())  # Convert to int\n            cropped_obj = frame.crop((x1, y1, x2, y2))  \n\n            obj_tensor = transform(cropped_obj).unsqueeze(0).to(device)  # Move object tensor to GPU\n\n            with torch.no_grad():\n                obj_feature = resnet(obj_tensor).view(-1).cpu().numpy()  # Move result back to CPU for numpy conversion\n\n            object_features.append(obj_feature)\n            bounding_boxes.append([x1, y1, x2, y2])\n    \n    # Debugging: Print detected objects\n    print(f\"Detected {len(object_features)} objects with scores: {detections['scores'].cpu().numpy()}\")\n\n    return object_features, bounding_boxes\n\n\n# Function to process each video and extract features\ndef process_video(video_folder):\n    frame_features_list = []\n    object_features_list = []\n    bounding_boxes_list = []\n\n    frame_files = sorted(os.listdir(video_folder))\n    for frame_name in frame_files:  # Process only the first 3 frames\n        frame_path = os.path.join(video_folder, frame_name)\n        frame = Image.open(frame_path).convert(\"RGB\")\n\n        # Extract frame-level features\n        frame_features = extract_frame_features_from_frame(frame)\n\n        # Extract object-level features\n        obj_features, boxes = extract_object_features_from_frame(frame)\n\n        object_features_list.append(obj_features)\n        bounding_boxes_list.append(boxes)\n        frame_features_list.append(frame_features)\n\n    return {\n        \"frame_features\": frame_features_list,\n        \"object_features\": object_features_list,\n        \"bounding_boxes\": bounding_boxes_list\n    }\n\n# Load existing features\ndef load_existing_features(pkl_file):\n    if os.path.exists(pkl_file):\n        with open(pkl_file, 'rb') as f:\n            return pickle.load  (f)\n    return {}\n\n# Save features\ndef save_features_to_pkl(all_video_features, pkl_file):\n    os.makedirs(os.path.dirname(pkl_file), exist_ok=True)\n    with open(pkl_file, 'wb') as f:\n        pickle.dump(all_video_features, f)\n\n# Process multiple videos with a progress bar\ndef process_videos(videos_folder, output_file):\n    all_video_features = load_existing_features(output_file)\n    video_list = sorted(os.listdir(videos_folder))\n\n    with tqdm(total=len(video_list), desc=\"Processing Videos\", unit=\"video\") as pbar:\n        for video_name in video_list:\n            if video_name in all_video_features:\n                pbar.update(1)  # Skip if already processed\n                continue\n\n            video_path = os.path.join(videos_folder, video_name)\n            if os.path.isdir(video_path):\n                video_features = process_video(video_path)\n                all_video_features[video_name] = video_features\n                save_features_to_pkl(all_video_features, output_file)  \n            pbar.update(1)  \n\n    return all_video_features\n\n# Example usage\nvideos_folder = '/kaggle/input/msvd-video-caption/validation'\noutput_file = '/kaggle/working/validation_all_video_features.pkl'\n\nall_video_features = process_videos(videos_folder, output_file)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm /kaggle/working/validation_all_video_features.pkl","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Path to the saved pickle file\npkl_file = \"/kaggle/working/validation_all_video_features.pkl\"\n\n# Load the features\nwith open(pkl_file, \"rb\") as f:\n    video_graph_features = pickle.load(f)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-11T18:06:42.544917Z","iopub.execute_input":"2025-03-11T18:06:42.545145Z","iopub.status.idle":"2025-03-11T18:06:44.201157Z","shell.execute_reply.started":"2025-03-11T18:06:42.545124Z","shell.execute_reply":"2025-03-11T18:06:44.198239Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"len(video_graph_features['bQJQGoJF7_k_162_169']['bounding_boxes'])","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-11T18:07:06.909930Z","iopub.execute_input":"2025-03-11T18:07:06.910220Z","iopub.status.idle":"2025-03-11T18:07:06.915378Z","shell.execute_reply.started":"2025-03-11T18:07:06.910199Z","shell.execute_reply":"2025-03-11T18:07:06.914690Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"210"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\n\ndef compute_iou(box1, box2):\n    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n    x1, y1, x2, y2 = box1\n    x1_p, y1_p, x2_p, y2_p = box2\n\n    # Compute intersection\n    inter_x1 = max(x1, x1_p)\n    inter_y1 = max(y1, y1_p)\n    inter_x2 = min(x2, x2_p)\n    inter_y2 = min(y2, y2_p)\n\n    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n    \n    # Compute areas\n    box1_area = (x2 - x1) * (y2 - y1)\n    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n    \n    # Compute IoU\n    iou = inter_area / float(box1_area + box2_area - inter_area)\n    return iou\n\ndef track_objects_across_frames(video_features, iou_threshold=0.5):\n    \"\"\"Assign tracking IDs to objects appearing across frames\"\"\"\n    tracking_info = {}\n    object_id = 1  # Initialize first object ID\n    \n    prev_frame_objects = {}  # Store objects in previous frame\n    \n    for frame_idx, (objects, boxes) in enumerate(zip(video_features[\"object_features\"], video_features[\"bounding_boxes\"])):\n        frame_objects = {}\n\n        for obj_idx, box in enumerate(boxes):\n            best_match_id = None\n            max_iou = iou_threshold  # Only assign if IoU is above threshold\n            \n            for prev_idx, prev_box in prev_frame_objects.items():\n                iou = compute_iou(box, prev_box)\n                if iou > max_iou:\n                    best_match_id = prev_idx\n                    max_iou = iou\n            \n            if best_match_id is not None:\n                frame_objects[obj_idx] = tracking_info[(frame_idx - 1, best_match_id)]\n            else:\n                frame_objects[obj_idx] = object_id\n                object_id += 1\n\n            tracking_info[(frame_idx, obj_idx)] = frame_objects[obj_idx]\n        \n        prev_frame_objects = {idx: box for idx, box in enumerate(boxes)}\n\n    return tracking_info\n    \ndef process_all_videos(all_video_features):\n    all_video_tracking_info = {}\n\n    # Iterate through all videos and track objects across frames\n    for video_id, video_features in all_video_features.items():\n        print(f\"Processing video: {video_id}\")\n        \n        # Track objects for the current video\n        tracking_info = track_objects_across_frames(video_features)\n\n        # Store the tracking information for the video\n        all_video_tracking_info[video_id] = tracking_info\n        print(\"added\")\n    return all_video_tracking_info\nall_video_tracking_info = process_all_videos(video_graph_features)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(all_video_tracking_info['bQJQGoJF7_k_162_169'])","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef build_graph_features(video_features, tracking_info):\n    frame_features_list = video_features[\"frame_features\"]\n    object_features_list = video_features[\"object_features\"]\n    bounding_boxes = video_features[\"bounding_boxes\"]\n\n    num_frames = len(frame_features_list)\n    num_objects = sum(len(objects) for objects in object_features_list)  \n\n    # Initialize adjacency matrix and feature tensor\n    adjacency_matrix = np.zeros((num_objects, num_objects))\n    feature_tensor = []\n\n    object_index_map = {}  # Maps (frame_idx, obj_idx) → global object index\n    global_obj_idx = 0\n\n    for frame_idx, objects in enumerate(object_features_list):\n        for obj_idx, obj_feature in enumerate(objects):\n            feature_tensor.append(obj_feature)\n            object_index_map[(frame_idx, obj_idx)] = global_obj_idx\n            global_obj_idx += 1\n\n    feature_tensor = np.array(feature_tensor)  \n\n    # ---- SPATIAL EDGES (within the same frame) ----\n    for frame_idx in range(num_frames):\n        if not bounding_boxes[frame_idx]: \n            continue  # Skip empty frames\n\n        bbox_centers = [( (box[0] + box[2]) / 2, (box[1] + box[3]) / 2 ) for box in bounding_boxes[frame_idx]]\n        bbox_centers = np.array(bbox_centers)\n\n        if bbox_centers.shape[0] == 0:\n            continue  \n\n        dist_matrix = cdist(bbox_centers, bbox_centers)\n\n        # Lower threshold to 100 or more for more edges\n        for i in range(len(bbox_centers)):\n            for j in range(i + 1, len(bbox_centers)):\n                if dist_matrix[i, j] < 100:  # Increase threshold for better connectivity\n                    obj1 = object_index_map[(frame_idx, i)]\n                    obj2 = object_index_map[(frame_idx, j)]\n                    adjacency_matrix[obj1, obj2] = 1\n                    adjacency_matrix[obj2, obj1] = 1\n\n    # ---- TEMPORAL EDGES (linking objects across frames) ----\n    for (prev_frame, prev_obj), track_id in tracking_info.items():\n        for (curr_frame, curr_obj), curr_track_id in tracking_info.items():\n            if curr_frame == prev_frame + 1 and curr_track_id == track_id:  \n                obj1 = object_index_map[(prev_frame, prev_obj)]\n                obj2 = object_index_map[(curr_frame, curr_obj)]\n                adjacency_matrix[obj1, obj2] = 1\n                adjacency_matrix[obj2, obj1] = 1\n\n    return adjacency_matrix, feature_tensor\n\n\n\ndef process_all_videos(video_object_tracking, video_object_feature):\n    all_video_graphs = {}  # Store graphs for each video\n    \n    for video_id in video_object_tracking.keys():\n        tracking_info = video_object_tracking[video_id]  # Object tracking data\n        video_features = video_object_feature[video_id]  # Extracted features\n\n        adjacency_matrix, feature_tensor = build_graph_features(video_features, tracking_info)\n        \n        all_video_graphs[video_id] = {\n            \"adjacency_matrix\": adjacency_matrix,\n            \"feature_matrix\": feature_tensor\n        }","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import networkx as nx\n# import matplotlib.pyplot as plt\n\n# def visualize_graph(adjacency_matrix, feature_tensor):\n#     # Create a graph from the adjacency matrix\n#     G = nx.from_numpy_array(adjacency_matrix)\n\n#     # Set up node labels based on feature indices (you can modify this to use more meaningful labels)\n#     labels = {i: f'Obj_{i}' for i in range(len(feature_tensor))}\n\n#     # Visualize the graph\n#     plt.figure(figsize=(10, 10))\n#     pos = nx.spring_layout(G)  # Layout for the nodes (spring layout)\n#     nx.draw(G, pos, with_labels=True, labels=labels, node_size=500, node_color='skyblue', font_size=10, font_weight='bold', alpha=0.6)\n    \n#     plt.title(\"Video Graph Visualization\")\n#     plt.show()\n# # Example usage: visualize the graph for a specific video\n# video_id = \"bQJQGoJF7_k_162_169\"  # Replace with a specific video ID\n# adjacency_matrix = all_video_graphs[video_id][\"adjacency_matrix\"]\n# feature_tensor = all_video_graphs[video_id][\"feature_matrix\"]\n\n# visualize_graph(adjacency_matrix, feature_tensor)\nall_video_graphs","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\n\n# Load the Q&A CSV file\nqa_df = pd.read_csv('/kaggle/input/msvd-question-and-answer/MSVD_QandA.csv')  # Adjust the path to your CSV file\n\n# Prepare the Q&A pairs (video_id, question, answer)\nqa_pairs = []\nfor _, row in qa_df.iterrows():\n    qa_pairs.append((row['video_id'], row['question'], row['answer']))\n\n# Assuming 'video_features' is a dictionary with video features as provided\n# Example structure:\n# video_features = {\n#     'bQJQGoJF7_k_162_169': {\n#         'adjacency_matrix': np.array(...),\n#         'feature_matrix': np.array(...),\n#     },\n#     ...\n# }\n\n# Dataset class for loading the video features and Q&A pairs\nclass VideoQADataset(Dataset):\n    def __init__(self, video_features, qa_pairs, tokenizer):\n        self.video_features = video_features\n        self.qa_pairs = qa_pairs\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.qa_pairs)\n    \n    def __getitem__(self, idx):\n        video_id = self.qa_pairs[idx][0]\n        question = self.qa_pairs[idx][1]\n        answer = self.qa_pairs[idx][2]\n        \n        # Get video feature matrices\n        video_feature = self.video_features[video_id]['feature_matrix']\n        adjacency_matrix = self.video_features[video_id]['adjacency_matrix']\n        \n        # Ensure they are numpy arrays and convert to tensors if needed\n        video_feature = torch.tensor(video_feature, dtype=torch.float32)\n        adjacency_matrix = torch.tensor(adjacency_matrix, dtype=torch.float32)\n        \n        # Tokenize question and answer\n        question_enc = self.tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n        answer_enc = self.tokenizer(answer, return_tensors='pt', padding=True, truncation=True)\n        \n        return video_feature, adjacency_matrix, question_enc, answer_enc\n\n\n# Model definition\nclass VideoQAModel(nn.Module):\n    def __init__(self, feature_dim, hidden_dim):\n        super(VideoQAModel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')  # Pre-trained BERT model\n        self.fc = nn.Linear(feature_dim, hidden_dim)  # Fully connected layer to map features to hidden state\n        self.decoder = nn.Linear(hidden_dim, 1)  # Change to appropriate size for the answer prediction\n    \n    def forward(self, video_features, adjacency_matrix, question_ids):\n        # Process video features (simplified example, can modify as needed)\n        feature_embeds = torch.mean(video_features, dim=1)  # Example aggregation\n        adj_embeds = torch.mean(adjacency_matrix, dim=1)  # Example processing for adjacency matrix\n        \n        # Question embedding from BERT\n        question_outputs = self.bert(**question_ids)\n        question_embed = question_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        \n        # Combine video and question embeddings\n        combined_features = torch.cat((feature_embeds, adj_embeds, question_embed), dim=-1)\n        \n        # Pass through fully connected layers\n        hidden = torch.relu(self.fc(combined_features))\n        output = self.decoder(hidden)\n        \n        return output\n\n# Initialize tokenizer for BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create the dataset and dataloader\ndataset = VideoQADataset(video_features, qa_pairs, tokenizer)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Model initialization\nfeature_dim = 2048  # Adjust based on the size of your video features\nhidden_dim = 512\nmodel = VideoQAModel(feature_dim=feature_dim, hidden_dim=hidden_dim)\n\n# Optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nloss_fn = nn.MSELoss()  # Use appropriate loss function for your task\n\n# Training loop\nepochs = 10  # Adjust based on your need\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in dataloader:\n        video_features, adjacency_matrix, question_ids, answer_ids = batch\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(video_features, adjacency_matrix, question_ids)\n        \n        # Compute the loss\n        loss = loss_fn(outputs, answer_ids['input_ids'])  # Adjust according to your output format\n        total_loss += loss.item()\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n\n# Save the model (optional)\ntorch.save(model.state_dict(), 'video_qa_model.pth')\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_all_videos(video_object_tracking, video_object_feature):\n    all_video_graphs = {}  # Store graphs for each video\n    \n    for video_id in video_object_tracking.keys():\n        tracking_info = video_object_tracking[video_id]  # Object tracking data\n        video_features = video_object_feature[video_id]  # Extracted features\n\n        adjacency_matrix, feature_tensor = build_graph_features(video_features, tracking_info)\n        \n        all_video_graphs[video_id] = {\n            \"adjacency_matrix\": adjacency_matrix,\n            \"feature_matrix\": feature_tensor\n        }\n\n    return all_video_graphs\nall_video_graphs = process_all_videos(all_video_tracking_info, video_graph_features)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(all_video_graphs['bQJQGoJF7_k_162_169']['adjacency_matrix'])","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dictionary to store tensorized data\nvideo_graph_tensors = {}\n\n# Iterate through all video graph data\nfor video_id, video_data in all_video_graphs.items():\n    adjacency_matrix = video_data['adjacency_matrix']\n    feature_tensor = video_data['feature_matrix']\n    \n    # Convert adjacency matrix and feature tensor to PyTorch tensors\n    adj_matrix_tensor = torch.tensor(adjacency_matrix, dtype=torch.float32)\n    feature_tensor = torch.tensor(feature_tensor, dtype=torch.float32)\n    \n    # Store the tensors back in the dictionary\n    video_graph_tensors[video_id] = {\n        'adjacency_matrix': adj_matrix_tensor,\n        'feature_tensor': feature_tensor\n    }\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_graph_tensors['bruzcOyIGeg_4_12']['feature_tensor'].shape","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCNLayer, self).__init__()\n        self.weights = nn.Parameter(torch.randn(in_features, out_features))\n\n    def forward(self, adjacency_matrix, feature_tensor):\n        # Ensure that adjacency_matrix is a tensor if it's not\n        adjacency_matrix = torch.tensor(adjacency_matrix, dtype=torch.float32)\n        \n        # Apply graph convolution operation: A * X * W\n        support = torch.matmul(feature_tensor, self.weights)  # X * W\n        out = torch.matmul(adjacency_matrix, support)  # A * (X * W)\n        return out\n\nclass VideoGraphNetwork(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features):\n        super(VideoGraphNetwork, self).__init__()\n        self.gcn1 = GCNLayer(in_features, hidden_features)\n        self.gcn2 = GCNLayer(hidden_features, out_features)\n\n    def forward(self, adjacency_matrix, feature_tensor):\n        # First GCN layer\n        x = self.gcn1(adjacency_matrix, feature_tensor)\n        x = F.relu(x)\n\n        # Second GCN layer\n        x = self.gcn2(adjacency_matrix, x)\n        \n        return x\n\n\n\n# Define the model\nmodel = VideoGraphNetwork(in_features=3, hidden_features=5, out_features=2)\n\n# Forward pass\noutput = model(video_graph_tensors['bb6V0Grtub4_174_185']['adjacency_matrix'], video_graph_tensors['bb6V0Grtub4_174_185']['feature_tensor'])\nprint(output)\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CLIP with yolo","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom torchvision import models, transforms\nimport pickle\nimport clip\nfrom ultralytics import YOLO\nfrom tqdm import tqdm  # Import tqdm for progress bar\n\n# Suppress YOLO logs\nimport logging\nlogging.getLogger(\"ultralytics\").setLevel(logging.CRITICAL)\n\n# Load CLIP model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device)\n\n# Load YOLO model\nyolo_model = YOLO(\"yolov8s.pt\")  \n\n# Load ResNet for object feature extraction\nresnet = models.resnet50(pretrained=True)\nresnet.eval()\n\n# Transformation for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Function to extract frame-level features using CLIP\ndef extract_frame_features_from_frame(frame):\n    frame_tensor = preprocess(frame).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features = clip_model.encode_image(frame_tensor)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n    return image_features.cpu().numpy()\n\n# Function to extract object-level features using YOLO\ndef extract_object_features_from_frame(frame):\n    object_features = []\n    bounding_boxes = []\n\n    # Run YOLO object detection (disable console output)\n    results = yolo_model(frame)\n\n    # Process YOLO detections\n    for result in results:\n        for box in result.boxes.data.cpu().numpy():\n            x1, y1, x2, y2, score, class_id = box  \n            if score > 0.2:  # Filter low-confidence detections\n                cropped_obj = frame.crop((x1, y1, x2, y2))  \n                obj_tensor = transform(cropped_obj).unsqueeze(0)  \n\n                with torch.no_grad():\n                    obj_feature = resnet(obj_tensor).view(-1).numpy()  \n\n                object_features.append(obj_feature)\n                bounding_boxes.append([x1, y1, x2, y2])\n\n    return object_features, bounding_boxes\n\n# Function to process each video and extract features\ndef process_video(video_folder):\n    frame_features_list = []\n    object_features_list = []\n    bounding_boxes_list = []\n\n    frame_files = sorted(os.listdir(video_folder))\n    for frame_name in frame_files[:3]:  # Process only the first 3 frames\n        frame_path = os.path.join(video_folder, frame_name)\n        frame = Image.open(frame_path).convert(\"RGB\")\n\n        # Extract frame-level features\n        frame_features = extract_frame_features_from_frame(frame)\n\n        # Extract object-level features\n        obj_features, boxes = extract_object_features_from_frame(frame)\n\n        object_features_list.append(obj_features)\n        bounding_boxes_list.append(boxes)\n        frame_features_list.append(frame_features)\n\n    return {\n        \"frame_features\": frame_features_list,\n        \"object_features\": object_features_list,\n        \"bounding_boxes\": bounding_boxes_list\n    }\n\n# Load existing features\ndef load_existing_features(pkl_file):\n    if os.path.exists(pkl_file):\n        with open(pkl_file, 'rb') as f:\n            return pickle.load(f)\n    return {}\n\n# Save features\ndef save_features_to_pkl(all_video_features, pkl_file):\n    os.makedirs(os.path.dirname(pkl_file), exist_ok=True)\n    with open(pkl_file, 'wb') as f:\n        pickle.dump(all_video_features, f)\n\n# Process multiple videos with a progress bar\ndef process_videos(videos_folder, output_file):\n    all_video_features = load_existing_features(output_file)\n    video_list = sorted(os.listdir(videos_folder))\n\n    with tqdm(total=len(video_list), desc=\"Processing Videos\", unit=\"video\", leave=True) as pbar:\n        for video_name in video_list:\n            if video_name in all_video_features:\n                pbar.update(1)  # Skip if already processed\n                continue\n\n            video_path = os.path.join(videos_folder, video_name)\n            if os.path.isdir(video_path):\n                video_features = process_video(video_path)\n                all_video_features[video_name] = video_features\n                save_features_to_pkl(all_video_features, output_file)  \n            pbar.update(1)  \n\n    return all_video_features\n\n# Example usage\nvideos_folder = '/kaggle/input/msvd-video-caption/train'\noutput_file = '/kaggle/working/train_all_video_features.pkl'\n\nall_video_features = process_videos(videos_folder, output_file)\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example labels (you can expand this list)\nlabels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a person\"]\ntext_inputs = clip.tokenize(labels).to(device)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_graph_features['eyhzdC936uk_15_27']['object_features'][0][0].shape","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction with CLIP","metadata":{}},{"cell_type":"code","source":"import clip\nimport torch\nfrom PIL import Image\n\n# Load the CLIP model and the preprocessing transform\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device)\n\n# Example labels (you can expand this list)\nlabels = [\"a photo of a cat\",\"a boy playing with dog\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a person\"]\ntext_inputs = clip.tokenize(labels).to(device)\n\n# Assuming 'video_graph_features' contains the features already extracted from the video frame\nframe_features = video_graph_features['eyhzdC936uk_15_27']['frame_features'][2]  # Extracted CLIP features\n\n# Normalize the extracted frame features\nframe_features = torch.tensor(frame_features).to(device)  # Convert to tensor\nframe_features /= frame_features.norm(dim=-1, keepdim=True)\n\n# Get the text features\nwith torch.no_grad():\n    text_features = model.encode_text(text_inputs)\n\n# Normalize the text features\ntext_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Calculate similarity between frame features and text features\nsimilarity = (frame_features @ text_features.T)\n\n# Get the predicted label (the index with the highest similarity score)\nvalues, indices = similarity.topk(1)\npredicted_label = labels[indices[0]]\nprint(f\"Predicted label: {predicted_label}\")\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction with Resnet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\nfrom PIL import Image\n\n# Load pre-trained ResNet model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nresnet = models.resnet50(pretrained=True).to(device)\nresnet.eval()  # Set to evaluation mode\n\n# Preprocessing for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Example labels (you can expand this list with more labels)\nlabels = [\"a photo of a cat\", \"a boy playing with dog\",\"a photo of a dog\", \"a photo of a car\", \"a photo of a person\"]\n\n# Example function to get features from ResNet\ndef extract_resnet_features(frame):\n    frame_tensor = transform(frame).unsqueeze(0).to(device)  # Preprocess frame\n    with torch.no_grad():\n        features = resnet(frame_tensor)  # Get ResNet features\n    return features\n\n# Example: Load the image ('image.jpg') and extract features using ResNet\nimage_path = '/kaggle/input/msvd-video-caption/testing/eyhzdC936uk_15_27/003.jpg'  # Make sure this is the correct path to your image\nframe_image = Image.open(image_path).convert(\"RGB\")  # Open the image\n\n# Extract ResNet features for the image\nframe_resnet_features = extract_resnet_features(frame_image)  # Get ResNet features\n\n# Normalize the ResNet features\nframe_resnet_features /= frame_resnet_features.norm(dim=-1, keepdim=True)\n\n# Example: Compare extracted features with predefined text features\n# Here we calculate cosine similarity between the ResNet features and the text features.\n\ndef cosine_similarity(a, b):\n    return F.cosine_similarity(a, b, dim=-1)\n\n# Create a dummy function for text encoding (in real scenario, this should be a pre-trained text model)\ndef encode_text(label):\n    # This is a placeholder for text encoding (you can use a pre-trained model like BERT or similar for this)\n    return torch.randn(1, 1000).to(device)  # Dummy random vector to simulate text features\n\n# Get features for each label (text description)\ntext_features = []\nfor label in labels:\n    text_feature = encode_text(label)  # Use a real model to encode text\n    text_features.append(text_feature)\n\n# Stack text features into a tensor\ntext_features = torch.stack(text_features).to(device)\n\n# Normalize text features\ntext_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Calculate similarity between image features and text features\nsimilarity = cosine_similarity(frame_resnet_features, text_features)\n\n# Find the label with the highest similarity\nvalues, indices = similarity.topk(1)\npredicted_label = labels[indices[0]]  # Get the predicted label\n\n# Output the predicted label\nprint(f\"Predicted label for the image: {predicted_label}\")\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Object Detection","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n# Load YOLOv8\nfrom ultralytics import YOLO\n\n# Load Faster R-CNN\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Load Models\nyolo_model = YOLO(\"yolov8n.pt\")  # YOLOv8 Nano (fast and lightweight)\nfasterrcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\nfasterrcnn_model.eval()\n\n# Load Image\nimage_path = \"/kaggle/input/msvd-video-caption/testing/eyhzdC936uk_15_27/060.jpg\"\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Preprocessing for Faster R-CNN\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\nimage_tensor = transform(image).unsqueeze(0)\n\n# Perform Detection with Faster R-CNN\nwith torch.no_grad():\n    fasterrcnn_preds = fasterrcnn_model(image_tensor)[0]\n\n# Perform Detection with YOLOv8\nyolo_preds = yolo_model(image_path)\n\n# Count objects detected\nnum_fasterrcnn_objects = sum(1 for score in fasterrcnn_preds[\"scores\"] if score > 0.5)\nnum_yolo_objects = len(yolo_preds[0].boxes)\n\n# Print Comparison\nprint(f\"Faster R-CNN detected {num_fasterrcnn_objects} objects\")\nprint(f\"YOLOv8 detected {num_yolo_objects} objects\")\n\n# Visualization Function\ndef draw_boxes(image, boxes, scores, color, model_name):\n    \"\"\"Draw bounding boxes on the image.\"\"\"\n    image = np.array(image)\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n    for i, box in enumerate(boxes):\n        if scores[i] > 0.5:  # Confidence threshold\n            x1, y1, x2, y2 = map(int, box)\n            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n            cv2.putText(image, f\"{model_name} {scores[i]:.2f}\", (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Draw YOLO Bounding Boxes\nyolo_img = draw_boxes(image, yolo_preds[0].boxes.xyxy, yolo_preds[0].boxes.conf, (0, 255, 0), \"YOLO\")\n\n# Draw Faster R-CNN Bounding Boxes\nfaster_img = draw_boxes(image, fasterrcnn_preds[\"boxes\"], fasterrcnn_preds[\"scores\"], (255, 0, 0), \"Faster R-CNN\")\n\n# Show Results\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(yolo_img)\nplt.title(\"YOLO Detection\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(faster_img)\nplt.title(\"Faster R-CNN Detection\")\n\nplt.show()","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Context aware","metadata":{}},{"cell_type":"code","source":"\nimport pickle\n\n# Path to the saved pickle file\npkl_file = \"/kaggle/working/train_all_video_features.pkl\"\n\n# Load the features\nwith open(pkl_file, \"rb\") as f:\n    video_graph_features = pickle.load(f)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_graph_features['-7KMZQEsJW4_205_208']['object_features'][2]","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null}]}