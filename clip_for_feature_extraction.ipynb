{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def extract_embedding(frame_path):\n",
    "    image = Image.open(frame_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: torch.Size([30, 512])\n"
     ]
    }
   ],
   "source": [
    "frames_dir = \"sampled_frames/\"\n",
    "frame_embeddings = []\n",
    "\n",
    "for frame_file in sorted(os.listdir(frames_dir)):\n",
    "    frame_path = os.path.join(frames_dir, frame_file)\n",
    "    embedding = extract_embedding(frame_path)\n",
    "    frame_embeddings.append(embedding)\n",
    "\n",
    "# Stack all embeddings into a single tensor\n",
    "frame_embeddings = torch.stack(frame_embeddings).squeeze()\n",
    "print(f\"Shape of embeddings: {frame_embeddings.shape}\")  # (30, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key frames: [tensor(376), tensor(376), tensor(376), tensor(389), tensor(39)]\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "frames_dir = \"frames/\"  # Directory containing all frames\n",
    "output_dir = \"unique_frames/\"  # Directory to save unique frames\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Assuming `frame_embeddings` is your tensor of shape (30, 512)\n",
    "# Step 1: Perform clustering\n",
    "# Cluster the frames into key scenes (e.g., 5 scenes)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(frame_embeddings)\n",
    "key_frame_indices = [frame_embeddings[kmeans.labels_ == i].mean(axis=0).argmax() for i in range(5)]\n",
    "\n",
    "print(f\"Key frames: {key_frame_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames in directory: 30\n",
      "Number of unique frames selected: 8\n",
      "Saved: frame_012.jpg as unique_frames/unique_frame_12.png\n",
      "Saved: frame_029.jpg as unique_frames/unique_frame_29.png\n",
      "Saved: frame_003.jpg as unique_frames/unique_frame_3.png\n",
      "Saved: frame_020.jpg as unique_frames/unique_frame_20.png\n",
      "Saved: frame_005.jpg as unique_frames/unique_frame_5.png\n",
      "Saved: frame_014.jpg as unique_frames/unique_frame_14.png\n",
      "Saved: frame_001.jpg as unique_frames/unique_frame_1.png\n",
      "Saved: frame_024.jpg as unique_frames/unique_frame_24.png\n",
      "Saved 8 unique frames as images to unique_frames/.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"unique_frames/\"  # Directory to save unique frames\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Assuming `frame_embeddings` is your tensor of shape (30, 512)\n",
    "# Step 1: Perform clustering\n",
    "n_clusters = 8  # Number of unique groups/scenes\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(frame_embeddings)\n",
    "\n",
    "# Step 2: Select one representative frame per cluster\n",
    "unique_frames = []\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    # Get indices of frames in the current cluster\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "    \n",
    "    # Find the frame closest to the cluster centroid\n",
    "    cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "    distances = np.linalg.norm(frame_embeddings[cluster_indices] - cluster_center, axis=1)\n",
    "    closest_frame_idx = cluster_indices[distances.argmin()]\n",
    "    \n",
    "    unique_frames.append(closest_frame_idx)\n",
    "\n",
    "# Debugging step: Check the length of `frame_files` and `unique_frames`\n",
    "frame_files = sorted(os.listdir(frames_dir))  # Ensure sorted order of frames\n",
    "\n",
    "# Debugging step: Check the length of `frame_files` and `unique_frames`\n",
    "print(f\"Total frames in directory: {len(frame_files)}\")\n",
    "print(f\"Number of unique frames selected: {len(unique_frames)}\")\n",
    "i = 0\n",
    "# Step 3: Save the selected unique frames as images\n",
    "for idx in unique_frames:\n",
    "    # Ensure the index is within bounds\n",
    "    if idx < len(frame_files):\n",
    "        frame_path = os.path.join(frames_dir, frame_files[idx])\n",
    "        \n",
    "        # Open the image using PIL\n",
    "        img = Image.open(frame_path)\n",
    "        \n",
    "        # Define save path\n",
    "        save_path = os.path.join(output_dir, f\"unique_frame_{i}.png\")\n",
    "        i += 1\n",
    "        # Save the image as a PNG\n",
    "        img.save(save_path)\n",
    "        print(f\"Saved: {frame_files[idx]} as {save_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Index {idx} is out of range for frame files.\")\n",
    "\n",
    "print(f\"Saved {len(unique_frames)} unique frames as images to {output_dir}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
